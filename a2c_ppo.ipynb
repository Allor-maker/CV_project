{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrandom\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnotebook\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datasets,models,transforms\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from torchvision import datasets,models,transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "import gymnasium\n",
    "import gym\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 32\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net,n_epoch = 2):\n",
    "    loss_fn=nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = torch.optim.Adam(net.parameters(),lr=learning_rate)\n",
    "    for epoch in tqdm(range(n_epoch)):\n",
    "\n",
    "        running_loss = 0.0\n",
    "        batchiter = iter(train_loader)\n",
    "        for i,batch in enumerate(tqdm(batchiter)):\n",
    "            x_batch,y_batch = batch\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = net(x_batch)\n",
    "\n",
    "            loss = loss_fn(y_pred,y_batch)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss+=loss.item()\n",
    "            if i % 500 == 499:\n",
    "              print('[%d, %5d] loss: %.3f' %\n",
    "                    (epoch + 1, i + 1, running_loss / 500))\n",
    "              running_loss = 0.0\n",
    "    print(\"обучение закончено\")\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self,batch):\n",
    "        return batch.view(batch.size(0),-1)\n",
    "    \n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.cv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.cv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.flatten = Flatten()\n",
    "        self.fc1 = nn.Linear(64 * dim * dim, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.cv1(x))\n",
    "        x = F.relu(self.cv2(x))\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.softmax(self.fc2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DataSelectionEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(DataSelectionEnv, self).__init__()\n",
    "        #загрузили наборы данных\n",
    "        self.train_data = datasets.CIFAR10(\"./root\", train=True,download=True,transform=transforms.ToTensor())\n",
    "        self.test_data = datasets.CIFAR10(\"./root\",train=False,download=True,transform=transforms.ToTensor())\n",
    "        #двумерный массив с индексами изображений по классам\n",
    "        self.indexes = self.class_select()\n",
    "        self.num_class = 10\n",
    "        model = SimpleCNN()\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(),lr=1e-3)#нужно создать модель и засунуть сюда это оптимайзер для самой светочной модели\n",
    "        self.entropy = nn.CrossEntropyLoss()\n",
    "        total_num_img = 5000\n",
    "\n",
    "    def sample(self,action):\n",
    "        index = []\n",
    "        for i in range(self.num_class):\n",
    "            num_img = int(action[i]*self.total_num_img)#на основе распределения вероятностей из action рассчитали количество изобьражений в каждом классе\n",
    "            indexes = np.random.choice(self.class_select[i],num_img,replace=True)#рандомно взяли индексы из списка индексов по классам в нужном количестве \n",
    "            index.extend(indexes)\n",
    "        return Subset(self.train_data,index)#взяли подмножество из train_data по индексам изображений\n",
    "    \n",
    "    def class_select(self):\n",
    "        ls = {i: [] for i in self.num_class}\n",
    "        for x, (_,label) in enumerate(self.train_data):\n",
    "            ls[label].append(x)\n",
    "        return ls\n",
    "    \n",
    "    def step(self,action):\n",
    "        subset = self.sample(action)\n",
    "        train_dataloader = DataLoader(subset,batch_size=32,shuffle=True)\n",
    "\n",
    "        self.model.train()\n",
    "        \n",
    "    def reset(self):\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_size, num_actions):\n",
    "        super(Actor,self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size,64)\n",
    "        self.fc2 = nn.Linear(64,num_actions)\n",
    "    def forwars(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.softmax(self.fc2(x),dim=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Critic,self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size,64)\n",
    "        self.fc2 = nn.Linear(61,1)\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.softmax(self.fc2(x),dim=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actor_critic(actor, critic, episodes, max_steps=2000, gamma=0.99, lr_actor=1e-3, lr_critic=1e-3):\n",
    "    optimizer_actor = optim.AdamW(actor.parameters(), lr=lr_actor)\n",
    "    optimizer_critic = optim.AdamW(critic.parameters(), lr=lr_critic)\n",
    "    stats = {'Actor Loss': [], 'Critic Loss': [], 'Returns': []}\n",
    "\n",
    "    env = gym.make('CartPole-v1')\n",
    "    input_size = env.observation_space.shape[0]\n",
    "    num_actions = env.action_space.n\n",
    "\n",
    "    for episode in range(1, episodes + 1):\n",
    "        state = env.reset()[0]\n",
    "        ep_return = 0\n",
    "        done = False\n",
    "        step_count = 0\n",
    "\n",
    "        while not done and step_count < max_steps:\n",
    "            state_tensor = torch.FloatTensor(state)\n",
    "            \n",
    "            # Actor selects action\n",
    "            action_probs = actor(state_tensor)\n",
    "            dist = Categorical(action_probs)\n",
    "            action = dist.sample()\n",
    "            \n",
    "            # Take action and observe next state and reward\n",
    "            next_state, reward, done, _,_ = env.step(action.item())\n",
    "            \n",
    "            # Critic estimates value function\n",
    "            value = critic(state_tensor)\n",
    "            next_value = critic(torch.FloatTensor(next_state))\n",
    "            \n",
    "            # Calculate TD target and Advantage\n",
    "            td_target = reward + gamma * next_value * (1 - done)\n",
    "            advantage = td_target - value\n",
    "            \n",
    "            # Critic update with MSE loss\n",
    "            critic_loss = F.mse_loss(value, td_target.detach())\n",
    "            optimizer_critic.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            optimizer_critic.step()\n",
    "            \n",
    "            # Actor update\n",
    "            log_prob = dist.log_prob(action)\n",
    "            actor_loss = -log_prob * advantage.detach()\n",
    "            optimizer_actor.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            optimizer_actor.step()\n",
    "            \n",
    "            # Update state, episode return, and step count\n",
    "            state = next_state\n",
    "            ep_return += reward\n",
    "            step_count += 1\n",
    "\n",
    "        # Record statistics\n",
    "        stats['Actor Loss'].append(actor_loss.item())\n",
    "        stats['Critic Loss'].append(critic_loss.item())\n",
    "        stats['Returns'].append(ep_return)\n",
    "\n",
    "        # Print episode statistics\n",
    "        print(f\"Episode {episode}: Actor Loss: {actor_loss.item():.4f}, Critic Loss: {critic_loss.item():.4f}, Return: {ep_return}, Steps: {step_count}\")\n",
    "\n",
    "    env.close()\n",
    "    return stats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
