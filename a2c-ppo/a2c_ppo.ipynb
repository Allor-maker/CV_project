{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1357,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import gymnasium\n",
    "import cv2\n",
    "from PIL import Image, ImageDraw\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1358,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-3\n",
    "NUM_CLASSES = 10\n",
    "IMG_SIZE = 128  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1359,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet():\n",
    "    def __init__(self):# 1. Параметры генератора\n",
    "        self.CELL_COUNT_RANGE = (1, 10)  # Количество клеток на изображение (минимум, максимум)\n",
    "        self.CELL_SIZE_RANGE = (5, 15)  # Диаметр клеток (минимум, максимум)\n",
    "        self.CELL_COLOR_RANGE = ((100, 0, 0), (255, 100, 100))  # Цвет клеток (минимум, максимум по BGR)\n",
    "        self.BACKGROUND_COLOR = (247, 131, 243)  # Розовый фон\n",
    "        self.OVERLAP_PROBABILITY = 0.1  # Вероятность перекрытия клетки с другой\n",
    "    \n",
    "    def random_color(self, color_range):\n",
    "        \"\"\"Генерирует случайный цвет в заданном диапазоне.\"\"\"\n",
    "        return tuple(random.randint(color_range[0][i], color_range[1][i]) for i in range(3))\n",
    "\n",
    "    def generate_blood_cell_image(self):\n",
    "        \"\"\"Генерирует одно изображение с клетками крови.\"\"\"\n",
    "        image = Image.new(\"RGB\", (IMG_SIZE, IMG_SIZE), self.BACKGROUND_COLOR)\n",
    "        draw = ImageDraw.Draw(image)\n",
    "        cell_count = random.randint(self.CELL_COUNT_RANGE[0], self.CELL_COUNT_RANGE[1])\n",
    "        cells = []  # Список координат и размеров клеток, чтобы отслеживать перекрытия\n",
    "\n",
    "        for _ in range(cell_count):\n",
    "            cell_size = random.randint(self.CELL_SIZE_RANGE[0], self.CELL_SIZE_RANGE[1])\n",
    "\n",
    "            # Попробуем найти позицию для клетки, чтобы избежать перекрытия (если OVERLAP_PROBABILITY низкая)\n",
    "            max_attempts = 100\n",
    "            for attempt in range(max_attempts):\n",
    "                x = random.randint(cell_size, IMG_SIZE - cell_size)\n",
    "                y = random.randint(cell_size, IMG_SIZE - cell_size)\n",
    "\n",
    "                # Проверяем, перекрывается ли новая клетка с существующими\n",
    "                overlap = False\n",
    "                if random.random() > self.OVERLAP_PROBABILITY: # Проверяем, нужно ли вообще проверять перекрытие\n",
    "                    for existing_x, existing_y, existing_size in cells:\n",
    "                        distance = np.sqrt((x - existing_x)**2 + (y - existing_y)**2)\n",
    "                        if distance < (cell_size + existing_size) * 0.7:  # Уменьшил коэфф. для допущения небольшого перекрытия\n",
    "                            overlap = True\n",
    "                            break\n",
    "\n",
    "                if not overlap:\n",
    "                    break # Нашли подходящую позицию\n",
    "\n",
    "            if overlap and attempt == max_attempts-1 :\n",
    "                #Если не нашли хорошую позицию, то игнорируем данную клетку\n",
    "                continue\n",
    "\n",
    "\n",
    "            cell_color = self.random_color(self.CELL_COLOR_RANGE)\n",
    "            draw.ellipse((x - cell_size, y - cell_size, x + cell_size, y + cell_size), fill=cell_color)\n",
    "            cells.append((x, y, cell_size))\n",
    "\n",
    "        return np.array(image), cell_count\n",
    "\n",
    "    def generate_dataset(self,num_images, output_dir=\"blood_cell_dataset\"):\n",
    "        \"\"\"Генерирует набор данных изображений и меток.\"\"\"\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "        images = []\n",
    "        labels = []\n",
    "\n",
    "        for i in range(num_images):\n",
    "            image, cell_count = self.generate_blood_cell_image()\n",
    "            images.append(image)\n",
    "            labels.append(cell_count)\n",
    "\n",
    "            # Сохранение изображений (опционально)\n",
    "            image_path = os.path.join(output_dir, f\"image_{i}.png\")\n",
    "            cv2.imwrite(image_path, cv2.cvtColor(image, cv2.COLOR_RGB2BGR))  # Convert to BGR for OpenCV\n",
    "\n",
    "        return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1360,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(subset):\n",
    "    return DataLoader(subset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1361,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, batch):\n",
    "        return batch.reshape(batch.size(0), -1)\n",
    "    \n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.cv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.cv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.flatten = Flatten()\n",
    "        self.fc1 = nn.Linear(64 * (IMG_SIZE//4) * (IMG_SIZE//4), 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 3, 1, 2).float()  # Изменяем порядок осей\n",
    "        x = F.relu(self.cv1(x))\n",
    "        x = self.pool(x)  # применяем пулинг после первой свертки\n",
    "        #print(f\"После первого пулинга: {x.shape}\")\n",
    "        x = F.relu(self.cv2(x))\n",
    "        x = self.pool(x)  # применяем пулинг после второй свертки\n",
    "        #print(f\"После второго пулинга: {x.shape}\")\n",
    "        #print(f\"Размер тензора после свертки: {x.shape}\")\n",
    "        x = self.flatten(x)\n",
    "        #print(f\"Размер тензора после flatten: {x.shape}\")\n",
    "        x = x.view(x.size(0), -1)  # или reshape\n",
    "        #print(f\"Размер тензора перед линейным слоем: {x.shape}\")\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1362,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BloodCellDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        #self.images = [cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) for img in images]  # Преобразуем в градации серого\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        #image = np.expand_dims(image, axis=0)  # Добавляем канал (1, H, W)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return torch.tensor(image, dtype=torch.int32), torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1363,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreloading():\n",
    "    def __init__(self, num_samples=1000):\n",
    "        self.dataset_generator = DataSet()\n",
    "        images, labels = self.dataset_generator.generate_dataset(num_samples)\n",
    "\n",
    "        self.dataset = BloodCellDataset(images, labels)\n",
    "    def get_train_data(self):\n",
    "        return self.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 128, 128, 3])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABJ4AAACMCAYAAAA9QmNpAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAQiZJREFUeJzt3Qd4HMXdBvB396ru1HuzLffeG8Y2YBvTe3USAuSjE5KQAAmEEAiBhNACgUACIYSW0AkxvRlj44px771IVu/S9dvvmRWyLUuyrutu9/3x7GPpdnd2xOg/upmbIimKooCIiIiIiIiIiCjC5EgnSEREREREREREJLDjiYiIiIiIiIiIooIdT0REREREREREFBXseCIiIiIiIiIioqhgxxMREREREREREUUFO56IiIiIiIiIiCgq2PFERERERERERERRwY4nIiIiIiIiIiKKCnY8ERERERERERFRVCRsx9OVV16JkpKSDq9JkoR77rmn1/JE0cPy1iaWqzaxXLWLZatNLNfExzLUD5Z14mMZ6gvLO8YdTzt37sR1112HAQMGwGq1IjU1FdOnT8fjjz8Oh8OBRLB161b8/Oc/x/HHH6/+DOIXZs+ePd1e/7///Q8TJkxQr+3bty/uvvtueL1e6IEWyrvda6+9hmnTpsFutyM9PV0t/y+++AJ6pKVybTd37lw1lm+66SbolRbK9e2338all16q/gw2mw1Dhw7FLbfcgvr6euiZFsq2HetibZWr3mNWC2XI98X6Ket33nkHp556KgoLC2GxWFBcXIyLLroIGzZsgB5ooQwZr/oq73hs7xhj8ZD3338fF198sVpRXX755Rg1ahTcbjcWL16M2267DRs3bsQzzzyDeLd06VL85S9/wYgRIzB8+HCsWbOm22s//PBDnHfeeTjppJPwxBNPYP369bjvvvtQWVmJp59+GlqmlfIWRE/0vffeq/5xFb3VHo9H/SNbWloKvdFSuR7Z8BFxrWdaKddrr71WfUN82WWXqW+QRJ375JNP4oMPPsC3336LpKQk6I1WylZgXay9ctVzzGqlDPm+WD9lLcorIyMDP/vZz5CdnY3y8nL885//xJQpU9Tfg7Fjx0KrtFKGjFd9lXdctneUKNu1a5eSnJysDBs2TCkrK+t0fvv27cpjjz0WdLpXXHGF0q9fvw6viR/n7rvvVqKlpqZGaWxsVL9+6KGH1Oft3r27y2tHjBihjB07VvF4PIdeu/POOxVJkpTNmzcrWqWl8l66dKlaXo8++qiid1oq13YOh0MpKSlR7r33XvWZP/7xjxW90VK5LliwoNNrL7zwgvrcZ599VtEbLZUt62JtlqteY1ZLZcj3xfop666Ul5crRqNRue666xSt0lIZMl71Vd7x2N6J+lS7Bx98EM3NzXjuuedQUFDQ6fygQYPU3vMjvfzyy5g4caL6aVdmZibmzZuH/fv3B/3spqYm3HzzzeqcStFrmZubqw4zE5+ktWttbcWWLVtQXV3dY3oiLykpKT1et2nTJvUQn+YZjYcHld14442iow9vvvkmtEpL5f3YY48hPz9fza8oN/Fz6ZWWyvXIn8nv9+PWW2+FXmmpXMWnckc7//zz1X83b94MvdFS2bIu1ma56jVmtVSGfF+sn7LuikhTTJPV8vRYLZUh41Vf5R2P7Z2odzzNnz9fnR8p5pMG4v7771eHtQ0ePBiPPvqoWgCff/45TjjhhKArtuuvv14dDnjhhRfiqaeeUv+Hi1+KI9/QrFixQh1uKIZ3R8rq1avVfydNmtThdTGkXMyJbj+vRVoqb5GPyZMnq8NSc3Jy1MpaVEKR/F1JFFoqV2Hfvn144IEH8Kc//UnT0zn0Vq5HE1MBBDEtQG+0VLasi7VZrnqNWa2XYVf0+r5Yi2Ut8lFVVaVOvbr66qvR2NiIOXPmQKu0WIY90Wu8arG898Vbeyeaw6kaGhrUIV3nnntuQNfv2bNHMRgMyv3339/h9fXr16tDOY98PZAha2lpaT0OJxNDvUMZ6nasIYrt5/bt29fp3OTJk5XjjjtO0SItlXdtba16XVZWljrkUpTpa6+9ppx22mnq63/7298UvdBSuba76KKLlOOPP77DM/U21U6L5Xq0q666Ss3ztm3bFD3RUtmyLtZmueo1ZrVchnxfrI+yHjp0qHqPOESd/Jvf/Ebx+XyKFmm1DAXGqz7K+6I4a+9EdXFx0QsuBDKsr33hKzEU7JJLLukwhEwMsRc9iQsWLMCvf/3rgJ8vdr1Zvnw5ysrK1F7a7oZ6t5VD5LSvdi+GyR1NrIzf/v9Fa7RU3u1TOWpqavDqq6+qO+8IYmHb0aNHqwvsid0O9EBL5SqI57/11ltqmnqmtXI92r///W91qPQvf/lLNX96oqWyZV2szXLVa8xqvQy7o8f3xVot6+eff1792Xbt2qV+LcrW5/NBlmO2UXrMaLUMe6LHeNVieS+Iw/ZOVDuexNaD7XMWA7F9+3b1f2Z3bzhMJlNQzxdzGq+44gr06dNHnXt5xhlnqMPhxBC6aGofyuZyuTqdczqd8THULQq0VN7tZSTyIBo47cQfVtHwEVuKiuGLYicerdNSuYptYH/605/ihz/8oTp1R8+0VK5HW7RoEa666ip162cxDFpvtFS2rIu1Wa56jVktl+Gx6PF9sVbLetq0aYe+FmvZiGk/wsMPPwyt0WoZ9kSP8aq18vbGaXsn6h1PosdObHkcCNFrKEmSuoWjwWDodD45OTmo54seyJkzZ+Kdd97BJ598goceekid4yh6KE8//XRES/tiZAcPHlR/eY4kXhNbj2qRlspbLA4nevVF7/PReROLvQl1dXW6aexopVxffPFFbN26FX//+9+xZ8+eDufEHxrxWvtimVqnpXI90tq1a3HOOeeo29+KBTCPXBhTL7RUtqyLtVmueo1ZrZZhT/T4vlgPZZ2RkYHZs2fjlVde0WzHk9bLsCt6jFetlfeL8dreifZcvmuvvVadT7hkyZIer33wwQfVa7du3RqVbQkrKiqUoqIiZfr06Uq4jjU3dsOGDeq5v/71rx1eLy0tVV8X2xlqlZbKW8xhFnN3XS5Xh9fvuusu9dmiPPVCK+Uq0m1fm6C745133lH0Qivl2m7Hjh1Kfn6+MmTIEKWyslLRMy2VLetibZarXmNWa2XYju+L9VPWRzrvvPOUpKQkRau0WoaMV22X991x2t6J+oRcMVffbrerOx9UVFR0Or9z5048/vjj6tcXXHCB2mP4u9/9rtP8RfG9WOMhUGK+cUNDQ4fXRM+e6Mk8cuhguFuJdmXkyJEYNmwYnnnmGTUf7cRK9aJn9MjpAlqjpfIW0zhEui+88EKHIabik50RI0Z0O/9Wi7RSrmJYuPgk4ehDEENaxddTp06FXmilXNt3wzrllFPUKVgff/yxuvuZnmmpbFkXa7Nc9RqzWirDQOn1fbGWyrqysrLTa2LUhNjB6+jdz7RES2UYKL3Gq5bKe16ctneiPp554MCB6oKR4o2jmAcs5iqK4dRutxtLlizBG2+8gSuvvPLQtWKh0DvuuEOtzM477zx1ga/du3er/4OuvfZadWvBQIhhZGLLRxEcY8eOVYe7ffbZZ1i5ciUeeeSRDtsSzpo1S10n4p577jlmmuIX4oknnlC//vrrr9V/xXaGYgqAOG666aZD14rhcWLouHhTJQpfDNsT14pf5Pb50FqkpfIWC9b+4x//wI9//GNs27ZNncrx0ksvYe/evep2m3qilXIVf0jF0ZX+/furedUTrZSrcNppp6mLnYo3DYsXL1aPdnl5eZg7dy70REtly7pYm+Wq15jVUhnyfbF+ylps5jBnzhyMGzdOnWIn1rcRmwF4PB51u3at0lIZMl71U97D4rW9E6uhVWJr3GuuuUYpKSlRzGazkpKSog4de+KJJxSn09nh2rfeekuZMWOGYrfb1WPYsGHq1n9HDmXraciaGJJ/2223KWPHjlWfJdIRXz/11FMhb0sohiN2N1zt6LwIYgjbuHHjFIvFohQXF6tbjrrdbkUPtFDe7cMcxbMzMzPVcpw6dary0UcfKXqllXI9Wm9vL9rbtFCuxxpOfOKJJyp6pYWyFVgXa69c9R6zWihDvi/WT1mLayZNmqRkZGSoW8UXFhYq8+bNU9atW6fogRbKkPGqr/KOx/aO9F0miIiIiIiIiIiIIirqazwREREREREREZE+seOJiIiIiIiIiIiigh1PREREREREREQUFex4IiIiIiIiIiKiqGDHExERERERERERRQU7noiIiIiIiIiIKCqMgV7ovK85OjmgoFl/kxyxtFiu8YPlqk2RLFeBZRs/GLPaxHLVpt4oV8Xrgbe6rNPrhsx8yGZLxPKjZ/wbq12MWW3i31j9lmvAHU9ERERERNQz57bV8FTsQ+s3n3c6lzTuBJjy+iJpxJReyRsRdcaYJYoudjwREREREYVJURR4aw6i+ev34CnbDcXZ0uV1jjVfwWm2wrl9DeyT58JUUAJJkmKeXyK9Y8wSxQ47noiIiIiIwuRrqEHd649DcTl6vFZxO+HetQGeA9uRccnNMOUUxSSPRHQYY5Yodri4OBERERFRGDzlewNuwB5JcbtQ98Zf4C7dGbW8EVFnjFmi2GLHExERERFRGBwblsLf0hDSvaLh27p6YcTzRETdY8wSxVZCdjz5/V54fa1HHQ51ni4RERERUay4dm1QFyYOh3vvZji2fBOxPBFR9xizRLGXUGs8iY6l+sZ1qKj+CvvK3uxwTpZMGD/yASRZC2BL4pxbIiIiIoousf265+CeoKfrdErH7YK3fC/8A0Z33rbdWQO07D38fVIhYMsP63lEesWYJeodCdPxVFO/CnUNa7F7/0tQFF+n837FjVUbfoH01NHISp+MkuJ5MBisvZJXIiIiItI+f2sTWlZ8EpG0xNQd68hpkHMK214QI/l3/Ruo2whULDp8YdZEIGscMPAyQE6Yt/JEcYExS9Q7jIkwyqmxeQs2bvsTnK7yHq+vb1yvHs2tuzBm2O963OpSpH+g6Vss2PfYodeGZ52GSfnf5zaZRERERBR7ogG75Wlg9+vA0R+41qxqO1oPAmNuB/h+laj3MWaJErvjqbllJ1au+yn8fldQ91VUL8S6LfdgxODbYDImd3vdwZaNeGnjlfD6nYde29e4CrJkwPi8i9V/iYiIiIhiwucEtj0P7H4NUPzdX3fgQwASMPIngNEeyxwS0ZEYs0SJ2fHk9zSj+eBH6terS18IutOpjYKK6gXISp+E4oKzu7xid/0yvLH1pg6dTurzFQ/e33mX+vXE/HkhPJuIIsXtbcSBus5DogvTZ8NqyuyVPBEREUVNw7a26To9UoAD7wM5U4DC2THIGBF1iTFLYXLroL0TVx1PLRUL0HLwY/i9rWitWIAGkxluqy2s4YilFe8jJ3MaLJbsTudWV7wOp7f7bTS/2v9XdjwR9aJNZX9HTfM6HGz4stO5/LRPkWkbhVHFP+mVvBEREUWc4gF2vBjcPXveArInAebUaOWKiLrDmKUwbdJJeycuOp58nia4Gzaias2d8Hub1NcUAA6jCUqYc2AbmjbB6a6E2ZzFNZuIEoBYd83ta8DOytexqexv6sYBXSlvWIzKxuVQ4MeQ/CtgNqRCkuSY55eIiPRLTk5DykkXoOnLt8NOyz7tDBjTs4HqlcHdWLcOCGl2AJH+MGYpHig6bO/0eq7dTTtQuugiHFx+zaFOJ6HJZEaz0RSRZ3yz/hffdWURUbzz+lvw7uoZ2FD6eLeV8JHTYjcffAbvrp4Ol7cuZnkkIiISJNkAY3YR5JSMsNKR7akw5RZDMnBtUaJoYsxSPPDqsL3Tqx1PrsatqFxzB7yOsq4viPIIpVE5Z8FiSOn2/NTCK6L6fCLqbHvFKyF1FG+veDkq+SEiIjoWc5/BsJQMDysNU+EAWAaMilieiKh7jFnqbdt12N4x9ubwMk/TTrgbt/RWFjAkczYuHf4UXtpwJRQc3vZSgoQ5Jb/E1AJ2PBHF0uayZ9V5zqHYWv68Gr2ji38a8XwREREdi33yyXDv3wZffXVIIyeSp5/13TdmYNQtwPqHAk9gyDWAOT3o51Ls2z5+xat+/fXqP6Kidl2H83mZYzB9/B3q17Jk5BIhUcaYpd6yWaftnV7reHLWrEDl2jtj8zC/D+7yjTCm5MGQnNvhVL/UqfjByH/isz1/OvTayOwzcVzhjyBLHDpJFIwq9y74xCKLR7AZ0pFqzOvxXre3AbUt6+FXQpvzLoah1rZsgMtTB4spvOHTREREwTCkZSPze7ei9tVH4aurDPy+1CxkXHozDMlpbS+ItTvShgOWLMBV03MC5jQgfTggR2Z5Coqe6vrNeOuzeVAUH3x+T6fRDhU167Bx52uQJAMuPPlV5GSM6LW86gFjlkLF9k4CdTyJHv/K1b8Cvuv1j/rzPA6UP3cOkgaeqB7JEy+DZGgLdvFpwoD043HtuHdjkhcirXH6mrCu+QP168V1z8Ph77hTZB/rWAyzz4JJsmJcyjndfoJ3sP4rlNZ/HlZeKhq/xv66jzEol7tREhFRbMlWG9LPvQ6OtV/BuX0t/M313V4rJSUjadhEWEced7gB2y5tMDD2TmD9nwBHRfcPNKUBo24FciZH8KegSKtv2o09ZQvx7eZn4PU5jnGlAp+/ba2X+QuvxoTh16Kk8ESkp/SPWV71hjFLgWJ7RyO72nUlyetFktej7mwXrpxdbZWIc+dC9fDU7ELGafdyCCtRBDqR51fdj22tC7u9Zr9zrXoYYILD34hpaZcx9oiISJOMGTlIOelCWAaPg7eqFE0L3ux0TfKMs2HK6wdz3yHdJyQapuN+C6y4BfA5u7hAAsbdCeROi+wPQCG9F6qp2ow1q57tdG7KjF/gk6W3ovKoaXU9aXVWYfHq+7Ft73ycc+JzsFo4LStaGLPUE7Z3ErjjqWrdb+Fz1x7zGpPih9nng8NgDGuRcdnrh73OJUL9kOZv/6P+m37yryGbkkJOm0jPXP5W/K/yd9jWuiig633w4KvaZyFDxuS0S2CQOMSYiIi0yVw0EKaCErUxezTZag9sJ6yM0cCs14GKxcC2fx5+veQCoM+ZgDmxpllojcfjgNvVhE/e/wmaG0vhdh/enbtdedkqtPhrALsESIra9xAM0WH11uffw/dP/4CN2ChjzFJX2N5J8I4nv6cpoFXcc1wOtJpM8ISx1lLuznqYnUdN6VN8aF71Mix9JsE+6tyQ0ybSq1ZfPT6ufgTbWr8K6j5RGX9e+6T69dS070ES8+KJiIg0um27wZ4aRgISYMkA+p7ddlDccLtbsGLJo9i6sfPomCO5nA0wiv/cRrhtbvis3qA7n1ocFSiv/hYFORPDyzT1iDFLR2J7J7Li/v9CusspxreFdK+10YWkxu4X7mpZ+yZ8ju7n8hJRZ2IxvQ+rH8Smls9CTuOL2qewrEFsI0pERESUWNNuli9+sMdOp6OZWk0wOoP/zN/tacKny27DgYplQd9LRKFhe0dHazy1S/W0LbJXZbUFPuVOUWBy+lC0qQYml6/by5y7F0NxtwBJnDcdLxSfF35HE+rm/63TuZTp58NcOBBSBNb9otC9XXHXMec4B0KBH1/VPQcZRkxN/576WkH6iSjKOBmldaFX8PmpM9An87Sw8kZErIuJIkkRm+koR70flWRInIKRkBYv+B22bQ5+UyIJEoytJnXOhy8puA2WGlv2o7p+C4rzjgv6uRQ8xiyxvaPDjifpiM6nJpMFTjG/9hgdUEaXV+10Kt5QBYM3tJFSFHuKxwVP1QE0r/gQ7tLtXY5yq5v/NGRbKtJPvRKG9BwYbGEMhaWQ1XkPRCQdr+JCs+/wtrNmYyqy7GNQXr8IvhC2GJUlEzKTR8NiZEcyUahYFxNFjiJ2KHPvgb/hPcCxseNJUx7krCsBUyEkQ3JvZZGC1Nx0EDXVWwJaMqS7zifZK8PnD37eSWPzfni8rTAZbSE9m3rGmKV2bO9oZKqdvWAuJEPgi3qLbqY0jxtFrU3IcDth83o6X6QoyNzXiPytdei3ppKdTglEUfxoWv4Bat9+HO4D27qfWqko8Lc0qNc1fvk6/M6WWGdV93a2LkOrry5i6ZW6NqDWc7hiH1ZwFUYU3RhSWsMKrsaoopsiljcivWFdTBTZ6VhK/bvwl/8RcKwXK5x2PDwH1XNK7StQ/ME3Pqh3iJFOYge7cIj1nkTnU7DWbX8R9Y27w3o2dY8xS+3Y3tFQx1Ny4RmQDcH31osOqCyXEzmOFhS2NHU4ihwuDBl0E5Lrutq+smupx18Pgy0r6HxQZCv5psX/RevaL4O6z7VrHeo//pfaUKLY2d66GC2+Y+9IGQyx7WiNZ2+H1wblzhOfBwaZkoRBuW1DWIkoeKyLiSLH37wE/opHoDR+1OO1Sssy+Cv/DH/9/JjkjUJXW70NO7a+F5G0xHpPIQ6aoihgzNKR2N7R2OLihqS8kO81KQpsPm+HI8WcDUvfKZADXK9JMifDXDwRkskacj4ofC3ffIzWdV+FtIC8e/9W1L//j6jki3qPUbbj3PFfY0zxz2GQLD0ONx1ReAPOG78EFiO3qCUKFetioghO1XHtAZximk6AHbLOrVBc26H4W6OdPQqDy9WApsb9EUlL8snseIoTjFnqDUYdtnd6peNJkiQUTH0W1sxJPV6b7LEi1Z3U4bB6Oy7sZkkbhYKp/4ClYAyyzn0UhpT8Yz/fkoKMU++GbcjJYf8sFDpfcz08FXtFjR9yGt6GKnU9EtIOUT+I+c9iKKkYhlqUPqfL6wrSTsTwgmvV4abiem5VShQa1sVEkaEoPigN70Jp+jT4mx3rodS8DMXHqatEscKYpd4i6bC902uLixtMqUjtNw/O2m86n/PLyHamqF+neKyQj+ofc8leOA1u+CUFVdZGJBedAZOtUD2XNGgWMs9+CNVvXAfF01UvtITMM+6DfeQ5UfrJKFDe2oNw7Tlq4b4g+eoq4Nq9Hqac4ojli+LH8IKr4fY2obB+VqdzhWknwWJK3F5/onjBupgoQhQPlIYPQ7+9ZSmktNMBgz2i2SKibjBmKQ4M10l7p1d3tbPlzkRKnwvRtP8ddWijpAB5jjQkeS0wKYZu77P4jeqhQIHda4HkMAB+HyC33WPtPx0F130Ex/bPUf/lo4fuS57wfSSP/x6MGX1i8vNR9/xuJxo+/3dE0mpZuxDmvsNhzi+JSHrUPZNkUecji+1BI8EAEww9VENmYwr6Z58fkecRUUesi4kix1/5ZMi7nR1O46+Qi/6Q0J9qU3RMGnEjMtMG93Y2NIUxS11he0eDHU+y0Ybs0XcDkgxv/Vaklu9FiidJ3Wo0EOI6s+iAWvMv+E2pUAadoaYlhq4Z0/sgedIV6tHhHimwtCnKFL+6K1JEknK1Qulqp0OKuNmZN2GnYzmq3Dsjkt7ktEvQP2lKRNIiohCwLiaKnEgsRqvupMTFf+KRxZKGlNQ+EVnnSTH423ZNCoLVkgGDwRz2s+kIjFnqAts7Gux4au8Iyhn5a8jL/wzZUxVaGqIT65sn4Vf8UIYcnkLHTiaiyIpGTDFOKRL2bF2I2srth74fMvZsJKeGvokFEUUP45USUWb2EAwaehZWr3w67LQ8Nk9QHU8ZqYNQkD0h7OcSUc/Y3tFox5P4tFVe9jCkPV+ElYwk0lnznDogThl8tijdiGWRiA47I/tXeKnsRvjhDSudHPNATEy9IGL5Iv1RFAUtjRVY9MEf0Fh3AM7W+kPnDuxahszcwZh+2m0c/k4UBxivpAVDhp+LfXu+RE3V5pDT8Jq98BsDn8JjMafh9OlPIDNtUMjPJKLgsL0Teb3+113+9hlIexYEO9q0S5LXAXnV05AOdl6wnIgio8gyCj8sfAp2Q2bIaWSb+uPKwmeQbmrbFIAoFM0NB/HeS9ejsnRDh0asUFe1Czs3foKvP3oQbldzr+WRiNowXkkLklMKkJU97Lv5FsET69OqnU5BtMAyUwchI3VgSM8jotCwvaO1jqemUki12yFFcF6spPgglS4FfO6IpUmRJxmMsPQfFZG0TLl9YUhOi0haFNhQ0WLraJydcxdSDcFPjehjHYvvFfwZZtkWlfyRPlSXb8Vnb98Bl7PxGFcpamP2my//Bq/HGcPcJQ7WxRQLeolXyTY5AmlMCrlTg2Jjxqy71ZFPoXQ6eW0e+JICH0HRN38mzjrhmYSdpiNGOtasehdVK97ocLSWhT5iLJIYs9Qdtnc0NtVOqt0BqWp95NPd/h4w5kqAC/DFLclohm30TLh2bwg7LUvJSBjTcxFJji2L4KnsuKCcbM9E8sTDa4jp3UDbcTgn9268Vv4LeJTAGgmFlhE4K+fXSDVyLQ8KT+nuFWisDWyB1+3rP8C46T+C0WSNer4SDetiigW9xKvYVl1pmN9hoeHSqmZs2nt4AePBxekoyU89Rhqnxd10w2U1H6LcuafDa1nmAszMOQ96bZBOnfFLSLIRWze+GdS6Tj5rYJ1OybYCTB93O/Kzx8Ni7v73JV45q/ei/Iu/q183bFkIxd/x57bmDoA1uz9MaXkoOvVnvZRLxiz1jO0dLXQ8KX7A0xK99F2NgCXxKmo9MRcOgm3MCWhd91XIaZgK+sM2blZE8uN3taD+w8eheF3wux2A76jdmSQZrt2r1C/T5t4AY1o+EpXP3wqfz3Hoe6MhGbJsCTqdvtZxuKzgr3i94jb1+1ZffaetR42SBRbZjiQ5FfPyH0WSgSMiKDxVZZuwceVrQd3z2Vu/wtmXP5uwnxpHE+tiiiZdxatkgZz7E7gOPg23pxXzv94Fh8sLt/fw38X9lU2wmAw4d/oAmE0G9WhjhJRxERAnUzJavA14ce/98Ck+OHwt8Ckd49AgGbC2YZG6w/Rl/W5HmikbemI22zF1+q0YP+k6fPL+T9DcWAq3u6nTdVZrBor6Ho/J026GV3Hi7S++B7/ig8vdCL+/4/9TWTapnUyyZMCFc15Fij0+fheC4XO1wOdoxI7nr4e3pfsd45yVu9QDskFtE+bOuBxGWwYkOcYdOIxZTWN7J770XseToxbyiseikrR4m2JYcAd8576ERONy7oXHffhTQWvSUBhNOdAiyWiCKa8EkvUbKM7WEBKQYS4eCtkc3qeifmczPJW70LLuY/iaqo697bijbZpA/Ud/Qcq0eTBmFMKQklgVt8dbh10HH0Z1w6eHXivImod+eTfAICcFlZZoFBRaR+Bnfd9Tv/+o5mG0eGs6XDPQNg3jUtpGJyRcI4Liks/ngccdXJ1x9JoydBjrYoomPcWr+BvnkEqwaFs2dmz/pMtrPF6/erzy2Vb0zUvBCWOLYLeaINnGQ0o9JS7+Tla5DuDtA39Fg6fj3/MjicZts7etnF7c8wdc3OdnyLf2g56YTEnqcd4lr6qLja9Z9Wyna2bO/h3M5pRD5XrlOYvVf1du/Cuq6zd1uDY7fQQmj/yx+nU8/B4Ey9NYhX3/ux9NO5YGfpPfh6plr6pH3/PvRsaY02P6szNmtRuzbO/En96datebD48zfr8LNRXPweHYCJdj66HXbclTYLEORlbulZr8JU4aOkl8xIOGT19S//gEwz7pFCRPOT2s5ys+L5pXvAXX3jXB3ed2oHHh8zDlDUTKjB/CYEuMXm2/342dZQ+ipvHzDq8frHkViuLDgIJbQ/o9a7/n9Oy2TwKIKLGwLiaKjNqand02YI+2r6IJlXWtGDD8SkjmfnHxPq/KVYr3yv6JOk9lwPe0+BrwXtk/cGbB/6EgqT/0RpRbdu4InHz6nwO6Vpgy6iZoic/ZjP3z/xhcp9NR9s9/QP1bkDUhtlOpGbPai1m2d+JTr3Y8URtF8eLg/nvR2ry807nW5hVobV4Fxe9Adv51kKT24Z3aYR00Xh1aW//Ji20NHqWHxeYNRiRPnAv7hJPDrvAbF70E9/51Id/vqdiJhi+eQcYZP1cXtlfniH/xI8B71DTSQfOAARe2dbf20tpjfsWLrft/jdqmrqfTlNe+pf47oOAXmvw9I+0QaymIQxFTtgMkG0xRzZMWsC6maNBTvHo8Tiz84o9B3bN040EUj5sBa1LoOydFSou3EW8deAL1nmOMOOxGtbsM75Q+je/3vU2dwhMPDXKKHZ+zCY3bvw4rDTG9umHrV8gcdyYkMQUvBhiz2otZtnfiV0Qn0lbWbse+8tUdjqq6jouCUmeVZY932el0mA/1te+gofZdaJGo6CwDxiLv2ofUBowxp7jL6wzpubD0H9123eRT1ekh4fA118BbdyCsNNR0Girh2/ER8PpY4PVxQNVKoG5Tx2PlPW3nPzgTaA5sgdVI233wkW4r4TZ+lNe+ibKa/8QwV0TByy0ahTHTfhjUPafPe1wzb6qihXUxRYOe4rWmejvq6/cFdU9TqwdVVdvR2+rcFXh+z+9CasC2a/TW4F977kWtuzyieaP45qzZh23P/l9E0mrcuhhlnz4Bn1jfLwYYs9qLWbZ3ND7iqb6pDGu3/w8rNv4HtQ17O5zLyRiIicMvxvih5yPVzpXdj+Zy7obLGUjnnILW5m+RnHqS2OsCWqO+wTQYkDLtbNgnzkXLyo87XZM0fCqMmZFZRNbbUIGmJf+Bv7n7hQ8D5veiYfl8pMipMEvdpedvW1C/aTew5BdA8Vxg8A8Akx2xEtinzUpQn0oT9VZ9kVc0GvbUPLQ0VvR4fZ9B02G2aK/ejAbWxRRpeorXBZ/d22F3rEB98ek9uOKqD9GbFlW/e2j9l3A4/a34vPJVXNLn5xHJF8W/g589dcyFxIOjoGrpf5Ax+lTYEf01bhmz2otZtnc02vGkKAreXfgblFVtwv6K1V1eI0Y8fbTkAWza9SmKc0fjrJl3t72xtaTCP+JSyJuC2+kkoHyJt5YTb0AicDm2weXcFtC1Lc3L4PVWi7X1oWVigdqU6edG9RneulJ4qzt2kobDDxvcSu4xGjtHqF7ddtSsBWY8Kd6VRywfRHpR0G8CTjzrLnz8+i3weV3dXlc8cBqOO/lmmCy2mOZPC1gXU6QwXuObeD8fjTRDHrUm8lO9F/LKNw6/1HcclJEnt33DWNX07w71jDFLuppqJ7acfGfBHeoop+46nY60r3wVlq5/CfO/uhtuj0NdW0HJGIRoUXJGRS1tSmyK3w/F2RL5dGGCogQRUvs/AZbGbnE6kyENUg99zbJkUbcaJUoE2QXDccb3n1BHUhhNHXdUS7JnoqDfRLWxa0vO6rU8Uvf0WhfrFeM1fm1oXIKtTasilt6els34pu6z4G/0OIGmKhjevBOG//0e8r41h4+lr8Dwr+uBql2AoyFieaXw1Hz7Lhq3LYp4ujteaNvdj7rGmO0e2zsaG/EkOp0++PoPWLnp1aDuE6vIL13/orpg3GnTfgWTPQ9KSjGkpvDXdjjSHrsRBRJXTqeuiW24m1e+HfF0nUoJLFI5zAh0rrUCNO4AGnYCaQMRbf3yb4RfcaOs5t9dnpckE/rmXYf8zAuinheiSBCfzGXmDsJF1/4HW9fOR+WB9YfOjT3+cqRmdL1GEcUHvdbFeqWHeO1bcjzq6naHcN909Cav3wOf4o1Yen744FU8wd3kdkBe+m/Im7/o8rTk8wA+D4xv/xZKdn/45twAZBRFJsMUMsXrVneiizS/K/IfSnSFMau9mGV7J36F1DfT6qzD8g0vh/zQJWufx0kTb4QpexiU/PER73hakp2Es+We+jrjg9U2EtakkXA6NvZ4bXLqiTCZuE6WptRuAJb9CpjxOIChUX9cv/ybAElGWXXn+C3JuwkFWfOingeiaBg69mz1IEqEuljvtBqvk6deh3Vr/h30NJipxyXG8hBR5WjstgF7NKl6N6T966Cw44nCxJjVZsyyvaORqXYiMF/+4PqwH9yehn/UZVAyBoawrFtnYomwFZkWHEixQRJbJScAs6UYFuuAtq2dj8mAJPsYGIxpiDVR5l5Pa4eDc7ojqGYN4IzUoozHJktG9M29FoVZP4BBth06+uf/HPlZFyfkLkIUOsXvgt/X2uFQ/M7ezhZ1g3WxduriUDBe45/JZMXsk+8J6p6ZJ94Ga1J6l+dEfPs9Dvg9rR0ORYwk0BJFgeET0ekbOPnb/wG13JmSwsOY1WbMsr0Tn4IeFFReswXVDXvCfnB1/S6U12xFftZQ+OY+BsNntwC1gS2y3V2n0zeZFrxfaMOlJTfDakicHWpyCn4Kn78ZzQ0LurnCiMyceUjLOAe9obF+B1Ys/AV8vrYFQY0mG6ac8AhS0zklIWJqxZSDaTF5lEG2oiT/pyjJ/8kRr0qshHXE72+F17EVrTUvw3vUaEuDuQT23OtgtA6EbEjttTxSZ6yLtVUXB4rxmjgkSUZW9mCkpBaiqbGsx+uTk/OQnTMEsmzo8Lrf3QxP/Q51Gmj90nuheDt2MCaVnAJrn1mQjFaYM4ch4dXsBRqD285dcjZCOrgVwPCoZYu0jzGr3Zhle0cDI54+WfYwXO6msB8sput9sfIvbd8YLfDN/C38eeNCTm9lpgUfFNrQxz4MWZbChPqlUrcaLrwNmTlXIMk2usO55NQTkJ13lXquN36m2qq1WLPsXvh8ogIVn6yLT9xbsGb571FXvSHm+dGslb+N6ePE75L4Y3v4SJx4ofCI7WNbq55D44Hb4XVsOBTX7YfPvVs911L5NBS/u7ezS99hXazNurgnjNfEIxqls0++G5OnXgtZNnV5jfi7O3HyVThpzp0oKBzf4Zzi96Jp/bOoW3Q76hbdAcXr6FTujj0fq+frl9wD58EVYee5IKkEOZbITYHJMOWiT9LggK+Xl78GydP9bofdMSx6Puh7KLJsRSNhyekf8XSzJp6HWGHMajdm2d6JL/GzDJI9F/7jboG0+D6gZhukACbftYfzyiwLPsuzIdvaB+cW34gMy7HXQWqbmtAxffHL2Jtk2Yys3B/C4z4ZHk/FodfNln4wGjNinh/x/0h8ur7+m4fgaD2cn3atzaVY982DmDDtXiSn9kuoQJatybCNPgWt6z+JaLoW6QCMqI9omkSRjuuWiifhbPigx2tdjV9AUbxIKbij1+tHPWNdrN+6mPGauIqKJ6KwaAKKiidjz+6vsHb14XVGRoy8AIOGnILCovGdykp0NDasegzOfYHtLuV31aPx28chTb4N5pyxIcd/vlU0YotR5SpFJGSYc1FsC7wRS4nLXjwS1uwSuKqCX6A7XjqeBMYsY5b01PEk2PPgm/Mw0FIBw6J7215rLoPk77hqv8MgodkoodUg4+WSFFjNacgwZeCKAffAYkg65iMczZVobtiL9Yv/dOi19NyRGDLhKiTZ82AwWtCbTOYC9eht4lP1tikd3a8f4Wg5iOVf/gyzznoDBoM5rDfXDTUVh9YqEcNX07JyES2SwQhDeuQXaZfhgCxpbA43aYbid6Cl6h/fNWIDWxfI3bQITZCQnPdTyNx2tlewLtZnXcx4TXyiQSkaqvn5ozFpyjWHXjcYTOpxNLEOTNPap+Hc93lQz/E7a1D39V3InPkAzNkjQ85vsjEdMmT41cUrQifWWE01ZYaVBiUWU2ouIKae+X0RSc9oz4TUC+0xxiyRnjqeBFHRpPWF76x/qN/Ka/8FxdXxk8uDdhPWp7e9uR4leqLTpqF/sviqe87WGlSXrsDezf9Fa1PH3uGaslVYWrYKJSMuRmrWYOT1PT7iP1Yial9HJNxrjqWqdA+qy/dh2cdvwv/dHyyT2YopJ5+P3OL+yMyNzu4HhuRsGNLy4GvoPIIgFBKcMEmJ/wk7aZendR2c9e8FeZcCd9NCuO2TYE07JUo5o56wLtZfXcx41Q7ZYITZcOy3236vE03rnoVj76ehPcTvURuy6cfdCUvexJCSmJ17CfyKH9/UhZiH74xOm4FT8y4P6h6l71gopZs6fdDcE3//SUHmjqKh+PRfqJ1O1SvfDDstc3oB+pxzJ5JyxcZLvYMx2zPGLGmj4+ko/rFXdnqt73dHoHxeFzYufRS15WuPed2eTW/AaE6G3+dCQf9ZIeSWglFbUYqv5r+M+uqDHV73uJ34+oP/ICu/D+ZcdA1S0rMi/mxTdl+kHP99NC78J/ytDWGm5keqvAZmqSpCuSOK/OiJ1trQ3xA669+H2T4Zci9M+6XoY10cXxivOpxOu+ZJOPd+Fl463lY0rPoz0ib8DJb8ySGlcVLOhZAlGStqPw7p/vHpJ2FWbvA7RimjTgVWvgW4g2zETjg3yBxStOSd8KOIdDwl5Q9ByoDQfn9jhTHLmKXQBL0QwIVzHkSSJQ3hSk7Kxrkn/h6x4HE1orZ8XUDXet3NqK/aBL/Wtr0MgdHU886ARlNow/ndLgc+/PcTnRo6R6op348PXnoMfl9khu521eCR7eEPLZXhhgnV4SVy4jNh54OoO4rigdcRWB3YFa9zi9oYpt7BulhfdTHjVX/c5d9EJB2/oxrepgMh32+UTZiZfR4mZ8yFFEQTQUzVGZM2E7NyL4FZtob0bN8ZtwV1vX/i+UBmMB9DUzQZ7Rnoc+5vIBlDn+5tzixGn7PvQCJgzDJmKQYdT3ZrJgYUhb/V8IDiabAnxWY+6aovfhPwGgnCge0forpsFfTMYLBi6omPwpZc3O019pS+mHrSn7vdAeJYDu7dDmdLz7sjtjY1oOLATkRL+twbYMwNfTcOAxqRLi+GJAX++9Ula0549xORJrEuDgzrYkpU3rrtUPyR+7DT27gXfk9LyPebZDNm516KiRlz1OZpIEanTcfp+VfALIe4Lo8YbZGaCyWrX0CXK0lpUMROaj1Mh6LYkWQDMsedhaLTfoGUwccHPb0udcgMDLn6ObUDK94xZhmzFKOOJzEU76yZdyFcZ82MzZbFFXsXw+0Mfvh+2a7P4PXo9xNDUc4paf0xetKtsNkLu2zojJ50G5JT+oS0I8PyT98K6Dqx1sjKL95FtEgGE1JnXA77hHNgSMkO/D54YJc2IkVeDYMU5u9J0RzAlh9eGkSkSayLe7iPdTEluOatr0IJo9F5NMeej+Br7n4EYyBEXSLWj5mTewlm5VyMbHPnuifNlK2em5VzCU7Juyz8HTVt6fDNuRG+474Pxdr9CE7/hPPgP+H/oJSEti4ORY/4HciedD4GzHsI2VMuCegeY3IW+pzzawz4/iMw2tKRCBiz32HMUpBC6nZMtuXglONuxafLH1W3kQyG2IbytGm3w2aJTeVSc/BbdfpcsKoOLFPXeoLp2LvkaV1G1khMOP5eLPvyZ+paWYLRZMOEab+DPaX7T+ATicGeDtvIWTAXj0D9R49D8bpFK6vrUXJi1w4AqdMuhHlZ5/XHgpY7BZhyH5DET9mJqHusi4/CupgoqsS6MZMz2xapH5wyHk5fa4fzYnpOtiXCuzBnFkPJLIavz2hIB7dAXnJ4S3ul/2T4x54BiBEWHDUR18SOpQVzbkDG2NPU6nvXKzfD7+r4+5M18TxkjDsDstGKpLyBvZZXLWHMUrwL6bfAaDDjxAk3qJ1OC775K7wB7qZjMloxZ8rNmDH+ajU4KDGIT9Rnn/lGh9ckWXsViCE1F1kX3at+3br+E7hLNx91PkddBFflaQSS+wDN+0N/oGQAMkezoUMxYIBsKoTfUxbS3bIxD5CCn8ZFkcW6WC91MeOV4kumOS+2D8zq29aYHX7ERj+i3fBdhzPFP4PFBnvRSHUh7pG3vN/l1DxxUHQwZikehfyOVZYNmD35pzDIJhyoXI8NOz845vVjBp+N4twxmDn+mlAfSb1EDMUU0yAiaeCoyViz6MMAni1jwIgJEX1298+SDvXI28edoR7dMmQCM58Glt4G1HdsFAVs6BXA+NtDzC31xNtUgeZtnxz63pw1ELaS4NYd0ArZYEdy7vVoLA1tirMt+zIYTPHSKNcv1sX6qIsZr9pS7SzF3uZNh74vsg9GflJJr+YpIYhGq4EfUmvi71YYC473BsZsiBizvcKbQO2dsD8qPXHiDWhx1GL80PPUqXflNVs6nC/KGY3Zk3+CksIpsFljP3e3z5Cz1IXCXY6aoO4rGXlxQDsJUWhGTZ0dUGNHlmWMmHQS4lLGMGDaQ0DjLmDJzWIP7MDuKzgRGDwPyJ/ZtjgfRZT4dK1mwZ/gqtwCV9nqQ68bU/Jhzh2G7JPvgtEe+BoyWmG0DoE5+Xi4m5cEdZ/JNk49SJtYF8dnXcx41cbfog8P/AO17oOoch7etSrTUoBMcz7O6HMNDJL2RiwSJSrGLCUSJQHbOxGJHrE73YgBp6B/0VT4fB1X+TcYzEiypKK3pGQOQFJKflAdTwajFRm5oyFH+JNlOsxsScLJF1+Hhe/+Cx63q9tr5l56PSQ5jnvPRYMnfShgTgWWffeJuaOi85okYqtzYzKQ0g+Y8ReAnZpR4fc4UfvVI2hc9xagdNz63dtUrh5lVdtQ9P1XYLDFZlfNeCEbM5BS8Es0lt0HT0tg2wAbrUORUng3ZIMt6vmj3sG6OD7rYsZrYnP5HGoDdnfz+k7nal0H1ePNPY/i3L43IW3SLaj+aCP87uA3wulK8ojLYUzj6AyiYDBmKZH4E7S9E9Fu2yRLGuLR+JPuxoLXL+16gdIuFA06DdmFsZlSoOeht/2GjsGMM3+APVvWYM+W1WrPrWAwGNF36FgMHjMV+X0HIe6JT8oLZgDnLxbdz8DyOwDvUbsr9TsT6NO24B9FT+vOL9G49vVjXuNtLEPlR79BwQVPQW8kOQmpRfeisfQeeFpWHPNaU9IYpBb/HpJsjVn+KPZYF8cvxmvi+rbmsy4bsEcqa92BZZXzcWL+JREdcScZkzS59htRNDFmKZG0Jmh7Rxe/5bLBjL5Dz8G+rT1vBW215SCvb3zOi9SiASMnov+ICdi6ZgkUf1uPrdFoxqAxU8Pf5rM3iDwf90Bv50KXfM5GNG14J6Br3TU70bpnSdzOgY4mSTIgJf9WuJoWwdnwPnyuXR3Oy6YCJGVcBHPyFDZidYR1cXxivCaeOlcFdjetC+ja/S1b1Ck99qHz0LTub2E/25g2AOZcTrUkCgZjlhKJL4HbO7roeJL8XgwcdREK/BXw73qrwzkvZKz1ta38P3L6r5CaNQS2lAhvNUnHJBo1w8ZP7+1sUIJTPK1w7FsW0LW+pnK4KzfHTUUca7IxDUkZZ6mNVcXfcbtdSbLAYGYdqEesi+MT4zWxNHvrUeHcG9C11a5SNHqqkDPwLEACmtY922naRKDkpGxkHH8PDLbckO4n0ivGLCUSJYHbO5rueFKcNUDjbvhW/BpoLYdNDL2XOk63Ey+dYNinfm3wVUDyip1g+CaOiLTPYOKbHaJEwXjVLjHNxjbwXHVh/OaNL0Dxdb3eWncMKX2QecKDMFgzopZHIjqMMUsUPM12PCnuJvhX3QflwKfHvO7IGQT+b+4FrDmQp94POT8+egaJiIiISPsjDu2DL1C/VkdRBLguqTF9sLrYMRuwRLHFmCUKThxvURM6RfHDv+yXPXY6dclZBf/Ku6FUHV5glYjin5yUgfQpVwd0rSVvJOxDT4t6noiISF9yrMUYnnZcQNcOShmPQlvHhfttg85DypjrRLP2u6M7Egz2fKRP+SVM3BGLKGSMWUokcgK3dzQ34klxN8C/7HYoBxeFnkjrQfgWXgPD7BeAzJGRzB4RRYlstMBaOBayORl+d3P3F0oyzDlDYEorimX2iIhIB6wGO/KTSrC9cRW8iqfb6wySEblJfWEzpnZ4XZJk2AaejaSSuep6ELWLbofi7bi2l22AOH+K+vdMNiZF7Wch0gPGLCUSOYHbO5rqeFKctW3T68LpdGrnc8L35TWQpz8GOW9KJLJHRFFmG3ACsmbfgZoFD8DvaurymuThZyF77t0xzxsREenDuKzZagP264r/wo+uFh6WMCn7VEzNObPL+yXZAEm2q19nz3kyyrklIsYsJRJbgrZ3NNXxhJb9UA58Ern0PI1Qdr4OsOOJKGGkjDgLsiUZroPrUL/iuUOvW4smIGXU+bAPnpOY28MTEVHCmJh1CpKNGdjXsgkb65ccen1I6iQMSBmDoWl8b0kUTxizlEhSErC9o5mOJ8Xnhm/JrZFPt2wh/Hveg1xyVsTTJqLosA88CUn9piFlVNuij4JstsNg40KOREQUfV5PK/onDUWRuR/Gpc6E2ZysTskR03osBltvZ4+IjsKYpURjT7D2jmY6ntSdBBxVkU/W5wC8x5g/SURxOwdaTi/u7WwQEZHONDbsw8LPbkNdzbZDr82YdT/6Dzo97j6BJiLGLCUuOYHaO5rZ1U7Z8ZoY9hSdtEu/gOKsjkraRERERKQNTY0HsGThPR0asMLSr+7Fzm3zey1fRNQ1xixRbGhmxJN/j6gYlKikrZQvAVz1gDU7KukTERERUWLzuFuw4JOfo752R6dzPp8L3yx9WJ26M2DwmRxFQRQHGLNEsaOZjiciIiIiot7y0fyrumzAtnO7m9SRFWnpJcjOHRWTPNU7KuHyOjq8ZjZYkWHLi8nzieIZY5YodtjxREREREQUJr/P3eM1iuKDovijnpdmVz02Vy7H6rLPUN1S2uFcZlI+JhafiqE5k5FqzYx6XojiFWOWKHbY8US6V+vYhmWlD3Z6/aR+f4TNlNMreSIi0hvWxUSR8fmOV1DeuBt76zd1eb7WUY5Pt7+ALVXLkZ9cgrlDLo95HonoMMYs6YFmOp4kS3qUVngCYEoGJEO0Uqde4PU78fWB32NH3Xvw+z1w+uo6XVPWvAySZMDY3KswPu96GGRzr+SViEirWBeTllis6T1eYzTZIMumqDzf63Pjy12v45sDH8MfwIY7++u34EDDNvXa2YN+AJOBsUX6wpglih3N7Gonz3wSkKLTjyaPuRlSav+opE2xpSgKdtd/gqUH/oh1lf9Eq6eyy4aO4PDWqOeXlv4R31Y8jX2NX8U8v0REWsS6mLRo7hlPI69gYrfnk5KyMGPWfcjKGR7xZ3v9Hizc/SZW7P8goAZsOzGFaFXpp/hy56vw+FwRzxdRPGPMEsWOZjqeiAKxsfoVfLjzOqypfCao+5aVPoCPd92AHbXcVpWIKFysi0mLDEYLZsy+H0V9ZnRxzorjTrgLfUtmReXZTk8Llu97L+T7Vx74CK2epojmiSjeMWaJYkc7HU+yCfLU+yKerJQ/HVKf0yKeLsXelpo38PX+e+FTnCHd7/TWYsG+X/HTdiKiMLAuJi2z2/Nw/In3IDd/PAwGy6Fj9il/RnHfE6L23P9ufCL8NDY8oY5GJNITxixRbGhnjSdJBtKHAbZCoLUsMomK+bxZo9X1oyixuX0tqGhZA7c/vE8GnN46VDR/i6Lk47jOCBFRkFgXkx4k2bJwylnPikkxh14T65RJkhSV59W0lKGq5UDY6VS3lqKmtQzZ9qKI5IsoUTBmiaJPOyOeRAWRNgiGqX8AkvtEJr1hV8Ew6qaIpEW9R8ybXlbato5IJCwr+5O6zgg/YSAiChzrYtITWTZAlo2Hjmg1YIWFu9+AIwJTblzeVnyx4z8RyRNRomHMEkWXpjqeBCl3EgzTHgaM9vDSGXkD5BHXRixf1HsUxYf1VS9FNM1INZyIiPSCdTERERGRPmmu40mVMQKGuf8BLJnB3yubII24Xu10krhFpSa0eMo7DJ2NVAPK4a2KaJpERFrGupiIiIhInzTZ8SSGRkqpA2CY8RdIJecCpuTA7is4AdLwq2EYfRMksb4TaYLYOcmveCKaptje+8u9v45omkREWsa6mIiIiEifNLO4eFek7HEwZI+Df/9JgKcZSukXUMoWdLzIaIM8/o6264tOgmTJ6J3MEhERERERERFpjKY7ntrJfeaq/ypFswB3Q8eTkgGwF0V1ATkiIiIiomg4ZfAV2Fe3Ga2exrDSsRrtOH3YVRHLFxF1jTFLeqTJqXbdkSzpkFL6dTySi9npREREREQJKdmSjj7pQ8NOpzhtCJLN6RHJExF1jzFLeqSrjiciIiIiIq05dciPwk9j6I/4YSxRjDBmSW/Y8USad1K/P0KWIjur1GrIwHFFv4xomkREWsa6mCh6kkzJOHHAJSHfP6PkAtjNaRHNExF1jzFLeqOLNZ5I37KTRkS8j1WWTciwDo5omkREWsa6mCh6DLIR0/qdDQUKluz5L7z+wHaQNEhGHNfvbMzofz5kse4pEcUEY5b0hh1PpHmSJGNgxhnYXvvfiKU5OOPsiKVFRKQHrIuJoks0QmeUnA8ZMg427cLWqpXHvH5w9kQUpPTHjP4XxCyPRHQYY5b0hB1PpHliasdJfdumeGyteTPs9MblXYtpRXdwTjURURBYFxPFxvEl56LV04Thucdh6d75qGje0+F8tr1Ybez2yxjBqTpEcYAxS3rAjifSBasxHQX2ydhe+z/4FXfI6RglK4pTZsAoWyOaPyIiPWBdTBQbNlMKRuRNQ0nGqE5TeIyyETZzaq/ljYg6Y8yS1rHjiXRjVM4P4fLVY3nZIyE1eMyGFMwovhslaSdHJX9ERHrAupgodmzmlN7OAhEFgTFLWsWOJ9INMR1jYv5P1E/cDzavxJYgpnqMzb0aufZxGJZ1YVTzSESkdayLiYiIiPSFHU+kuwbPqJzLMSjjHHj8Tuyse6/He8bk/h+mFd0Ok8EekzwSEWkd62IiIiIi/ZAURVF6OxNERERERERERKQ9cm9ngIiIiIiIiIiItIkdT0REREREREREFBXseCIiIiIiIiIioqhgxxMREREREREREUUFO56IiIiIiIiIiCgq2PFERERERERERERRwY4nIiIiIiIiIiKKCnY8ERERERERERFRVLDjiYiIiIiIiIiIEA3/D9Q7K7Tmx0MrAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1500x300 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "NUM_IMAGES = 10\n",
    "data_set = DataPreloading(num_samples=10)\n",
    "data = data_set.get_train_data()\n",
    "dataloader = DataLoader(data, batch_size=10, shuffle=True)\n",
    "images, labels = next(iter(dataloader))\n",
    "print(images.shape)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_data(images, labels, num_samples=10):\n",
    "    \"\"\"Визуализирует несколько случайных изображений из набора данных.\"\"\"\n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=(15, 3))\n",
    "    for i in range(num_samples):\n",
    "        index = random.randint(0, len(images) - 1)\n",
    "        axes[i].imshow(images[index])\n",
    "        axes[i].set_title(f\"Cells: {labels[index]}\")\n",
    "        axes[i].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "visualize_data(images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSelectionEnv(gymnasium.Env):\n",
    "    def __init__(self):\n",
    "        super(DataSelectionEnv, self).__init__()\n",
    "        self.train_data = DataPreloading().get_train_data()\n",
    "        self.model = SimpleCNN()\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.indexes = self.class_select()\n",
    "        self.optim = optim.Adam(self.model.parameters(), lr=LEARNING_RATE)\n",
    "        \n",
    "        # Пространство действий: вероятности выбора изображений для каждого класса\n",
    "        self.action_space = gymnasium.spaces.Box(low=0, high=1, shape=(NUM_CLASSES,), dtype=np.float32)\n",
    "        self.observation_space = gymnasium.spaces.Box(low=0, high=1, shape=(NUM_CLASSES,), dtype=np.float32)\n",
    "    \n",
    "    def class_select(self):#возвращаем словарь из индексов принадлежащим классам\n",
    "        ls = {i: [] for i in range(1,NUM_CLASSES+1)} #создали пустой словарь на 10 классов\n",
    "\n",
    "        for x, (_, label) in enumerate(self.train_data):\n",
    "            label_value = label.item()\n",
    "            ls[label_value].append(x) #индекс каждого изображения из train_data положили в нужный класс в зависимости от метки\n",
    "\n",
    "        return ls\n",
    "    \n",
    "    def sample(self, action):#action - тензор распределения процентов изображений от каждого класса в выборке\n",
    "        action = F.softmax(torch.tensor(action), dim=-1).numpy()\n",
    "        index = []\n",
    "\n",
    "        for i in range(NUM_CLASSES):\n",
    "            num_img = int(action[i] * BATCH_SIZE) #определили количество изображений для i-го класса в выборке из batch_size изображений\n",
    "            indexes = np.random.choice(self.indexes[i+1], num_img, replace=True) #рандомно выбираем вычисленное количество изображений из изображений нужного класса, возвращаем их индексы\n",
    "            index.extend(indexes)#добавляем найденные индексы в выборку\n",
    "\n",
    "        return Subset(self.train_data, index)#состовляем subset \n",
    "    \n",
    "    def step(self, action):\n",
    "        # в этой функции мы создаем выборку на основе action и проверяем, насколько улучшилось или ухудшилось предсказание сети\n",
    "        action = np.clip(action + np.random.uniform(-0.05, 0.05, size=action.shape), 0, 1)\n",
    "        train_subset = self.sample(action) #subset - выбранное случайное подмножество из 32 элементов на основе распределения action\n",
    "        test_subset = self.sample(action) #subset - выбранное случайное подмножество из 32 элементов на основе распределения action\n",
    "\n",
    "        train_dataloader = get_dataloader(train_subset) #создали dataloader, выдающий этот batch из 32 элементов \n",
    "        test_dataloader = get_dataloader(test_subset) #создаем dataloader для тестирования\n",
    "\n",
    "        prev_acc, _ = self.evaluate(test_dataloader) #вычиляем текущую точность модели на тестовой выборке\n",
    "        self.train_model(train_dataloader) #тренируем модель на тренировчной выборке\n",
    "        new_acc, err_per_cl = self.evaluate(test_dataloader) #проверяем модель после тренировки на тестовой выборке\n",
    "\n",
    "        reward = new_acc #вычисляем награду\n",
    "        for i in range(NUM_CLASSES):\n",
    "            if err_per_cl[i]==1:\n",
    "                reward-=30\n",
    "            if err_per_cl[i]<1 and err_per_cl[i]>=0.9:\n",
    "                reward+=5\n",
    "            elif err_per_cl[i]<0.9 and err_per_cl[i]>=0.8:\n",
    "                reward+=10\n",
    "            elif err_per_cl[i]<0.8 and err_per_cl[i]>=0.7:\n",
    "                reward+=15\n",
    "            elif err_per_cl[i]<0.7 and err_per_cl[i]>=0.6:\n",
    "                reward+=20\n",
    "            elif err_per_cl[i]<0.6 and err_per_cl[i]>=0.5:\n",
    "                reward+=25\n",
    "            elif err_per_cl[i]<0.5:\n",
    "                reward+=50\n",
    "            \n",
    "        #reward = new_acc\n",
    "        return reward, err_per_cl\n",
    "    \n",
    "    def train_model(self, dataloader):\n",
    "        # на вход приходит dataloader выдающий batch изображений и ответов к ним\n",
    "        batch = next(iter(dataloader)) #получили батч из тренирогочного dataloader \n",
    "        images, labels = batch #разделили полученный батч на изображения и метки\n",
    "\n",
    "        self.optim.zero_grad() #обновили optimizer\n",
    "        output = self.model(images) #получили тензор(batch_size, NUM_CLASSES) с распределением вероятностей для каждого изображения\n",
    "        #print(output)\n",
    "        loss = self.criterion(output, labels-1) #вычислили ошибку с помощью CrossEntropyLoss\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "    \n",
    "    def evaluate(self,dataloader):#тестирование модели\n",
    "        self.model.eval()#перевели модель в оценочный режим\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        ls = [0]*10\n",
    "        ers = [0]*10\n",
    "        with torch.no_grad():\n",
    "            batch = next(iter(dataloader)) #получили из dataloader batch на batch_size изображений\n",
    "            images,labels = batch # получили изображения и ответы\n",
    "            output = self.model(images) #получили тензор(batch_size,NUM_CLASSES) с распределениями вероятснотей для каждого изображения\n",
    "            predicted = torch.argmax(output,dim=1) #получили тезор(batch_size) с предложенными значениями классов\n",
    "            correct += (predicted==labels).sum().item() #нашли количество правильных ответов\n",
    "            total = BATCH_SIZE #так как мы рассматриваем только один батч, следовательно общий размер - BATCH_SIZE\n",
    "\n",
    "            for i in range(len(labels)):\n",
    "                if predicted[i]==labels[i]:\n",
    "                    ls[labels[i]-1]+=1  \n",
    "                else:\n",
    "                    ers[labels[i]-1]+=1\n",
    "                    ls[labels[i]-1]+=1\n",
    "            error_per_class = [ers[i]/ls[i] for i in range(NUM_CLASSES)]\n",
    "\n",
    "        accuracy = (correct / total) if total > 0 else 0 # находим точность на данной выборке\n",
    "        print(accuracy)\n",
    "        return accuracy, error_per_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1366,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_size, num_actions):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, num_actions)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.softmax(self.fc2(x), dim=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1367,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1368,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actor_critic(env, actor, critic, episodes=100, max_steps=100, gamma=0.99, lr_actor=1e-3, lr_critic=1e-3):\n",
    "    optimizer_actor = optim.AdamW(actor.parameters(), lr=lr_actor)\n",
    "    optimizer_critic = optim.AdamW(critic.parameters(), lr=lr_critic)\n",
    "    EPS = 0.3 #значение epsilon для клиппинга обычно от 0.1 до 0.3\n",
    "    state = np.ones(NUM_CLASSES) / 2 # проценты точности на каждом классе\n",
    "    for episode in range(episodes):\n",
    "        step = 0\n",
    "        while step < max_steps:\n",
    "            print(step, state)\n",
    "            step += 1\n",
    "            state_tensor = torch.FloatTensor(state)#флотовый тензор с процентами точности\n",
    "            epsilon = 0.1  # Вероятность случайного выбора действия\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = np.random.dirichlet(np.ones_like(state))  # Случайное распределение\n",
    "                action_probabilities = torch.FloatTensor(action)  # Создаем фейковый тензор, чтобы избежать ошибки\n",
    "            else:\n",
    "                with torch.no_grad():  \n",
    "                    action_probabilities = actor(state_tensor)  \n",
    "                    action = action_probabilities.detach().numpy()\n",
    "\n",
    "            old_log_prob = torch.log(action_probabilities)\n",
    "            #print(action)\n",
    "            reward, next_state = env.step(action) #рассчитываем награду и next_state, где next_state - ndarray процента ошибок по классам на тестовой выборке из batch_size элементов\n",
    "            next_state_tensor = torch.FloatTensor(next_state)#создаем тензор от next_state\n",
    "            new_action_probabilities = actor(next_state_tensor)\n",
    "            new_log_prob = torch.log(new_action_probabilities)\n",
    "\n",
    "            value = critic(state_tensor) #критик предсказывает значение на основе предыдущего тензора \n",
    "            next_value = critic(next_state_tensor) #и предсказывает значение на основе нового тензора\n",
    "            \n",
    "            advantage = reward + (gamma * next_value) - value\n",
    "            print(advantage)\n",
    "            loss_critic = advantage.pow(2).mean()\n",
    "\n",
    "            #вычисляем потерю актера с учетом клиппинга\n",
    "            ratio = torch.exp(new_log_prob - old_log_prob)\n",
    "            surrogate_loss = torch.min(ratio * advantage, torch.clamp(ratio, 1 - EPS, 1 + EPS) * advantage)\n",
    "            entropy_bonus = -torch.sum(action_probabilities * torch.log(action_probabilities + 1e-6))  # Энтропия\n",
    "            \n",
    "            loss_actor = -surrogate_loss.mean()\n",
    "            loss_actor += 0.01 * entropy_bonus  # Добавляем в функцию потерь\n",
    "            \n",
    "            state = next_state\n",
    "\n",
    "            #вводим фиктивную раномерно распределенную выборку для более объективной оценки точности\n",
    "            if step == 99:\n",
    "                test_state = np.ones(NUM_CLASSES)/NUM_CLASSES\n",
    "                test_action_probabilities = torch.FloatTensor(test_state)\n",
    "                test_action = test_action_probabilities.numpy()\n",
    "                _,state = env.step(test_action)\n",
    "            #обновляем политику раз в 10 шагов\n",
    "            if step % 10 != 0:\n",
    "                optimizer_actor.zero_grad()\n",
    "                loss_actor.backward()\n",
    "                optimizer_actor.step()\n",
    "            else:\n",
    "                optimizer_critic.zero_grad()\n",
    "                loss_critic.backward(retain_graph=True)\n",
    "                optimizer_critic.step()\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]\n",
      "0.09375\n",
      "0.1015625\n",
      "tensor([500.2638], grad_fn=<SubBackward0>)\n",
      "1 [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]\n",
      "0.09375\n",
      "0.09375\n",
      "tensor([500.1412], grad_fn=<SubBackward0>)\n",
      "2 [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "0.09375\n",
      "0.09375\n",
      "tensor([500.1124], grad_fn=<SubBackward0>)\n",
      "3 [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]\n",
      "0.109375\n",
      "0.09375\n",
      "tensor([499.9292], grad_fn=<SubBackward0>)\n",
      "4 [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "0.09375\n",
      "0.09375\n",
      "tensor([500.1283], grad_fn=<SubBackward0>)\n",
      "5 [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "0.09375\n",
      "0.09375\n",
      "tensor([500.1652], grad_fn=<SubBackward0>)\n",
      "6 [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]\n",
      "0.1015625\n",
      "0.09375\n",
      "tensor([500.0777], grad_fn=<SubBackward0>)\n",
      "7 [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "0.09375\n",
      "0.09375\n",
      "tensor([500.1396], grad_fn=<SubBackward0>)\n",
      "8 [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "0.09375\n",
      "0.09375\n",
      "tensor([500.0906], grad_fn=<SubBackward0>)\n",
      "9 [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "0.1015625\n",
      "0.109375\n",
      "tensor([500.1071], grad_fn=<SubBackward0>)\n",
      "10 [1.0, 1.0, 0.6153846153846154, 1.0, 1.0, 1.0, 1.0, 0.5, 0.7692307692307693, 1.0]\n",
      "0.1484375\n",
      "0.1015625\n",
      "tensor([500.0339], grad_fn=<SubBackward0>)\n",
      "11 [0.9166666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9230769230769231, 0.15384615384615385, 1.0]\n",
      "0.0859375\n",
      "0.0703125\n",
      "tensor([500.1032], grad_fn=<SubBackward0>)\n",
      "12 [0.9166666666666666, 1.0, 1.0, 1.0, 1.0, 0.9166666666666666, 1.0, 0.7692307692307693, 0.6923076923076923, 1.0]\n",
      "0.1171875\n",
      "0.09375\n",
      "tensor([500.0197], grad_fn=<SubBackward0>)\n",
      "13 [0.5, 1.0, 1.0, 1.0, 1.0, 0.9230769230769231, 1.0, 0.8333333333333334, 0.7692307692307693, 1.0]\n",
      "0.1171875\n",
      "0.140625\n",
      "tensor([500.0837], grad_fn=<SubBackward0>)\n",
      "14 [0.16666666666666666, 1.0, 1.0, 1.0, 0.6923076923076923, 0.9166666666666666, 0.9166666666666666, 0.8333333333333334, 1.0, 1.0]\n",
      "0.078125\n",
      "0.078125\n",
      "tensor([500.1301], grad_fn=<SubBackward0>)\n",
      "15 [0.4166666666666667, 0.9166666666666666, 1.0, 1.0, 1.0, 1.0, 0.8461538461538461, 1.0, 1.0, 1.0]\n",
      "0.1640625\n",
      "0.1328125\n",
      "tensor([500.1053], grad_fn=<SubBackward0>)\n",
      "16 [0.5833333333333334, 0.9166666666666666, 1.0, 0.8333333333333334, 0.7692307692307693, 0.9166666666666666, 0.6923076923076923, 1.0, 0.9230769230769231, 1.0]\n",
      "0.1640625\n",
      "0.1171875\n",
      "tensor([500.1352], grad_fn=<SubBackward0>)\n",
      "17 [0.5833333333333334, 1.0, 1.0, 0.6923076923076923, 1.0, 0.9230769230769231, 0.7692307692307693, 0.9166666666666666, 0.9166666666666666, 1.0]\n",
      "0.03125\n",
      "0.046875\n",
      "tensor([500.1428], grad_fn=<SubBackward0>)\n",
      "18 [1.0, 1.0, 1.0, 0.8333333333333334, 0.9166666666666666, 1.0, 1.0, 0.75, 1.0, 1.0]\n",
      "0.046875\n",
      "0.046875\n",
      "tensor([500.0230], grad_fn=<SubBackward0>)\n",
      "19 [1.0, 1.0, 1.0, 0.75, 0.9230769230769231, 1.0, 0.8333333333333334, 1.0, 1.0, 1.0]\n",
      "0.078125\n",
      "0.046875\n",
      "tensor([500.0582], grad_fn=<SubBackward0>)\n",
      "20 [1.0, 1.0, 1.0, 0.8181818181818182, 0.9230769230769231, 1.0, 0.9090909090909091, 0.8461538461538461, 1.0, 1.0]\n",
      "0.078125\n",
      "0.140625\n",
      "tensor([499.9939], grad_fn=<SubBackward0>)\n",
      "21 [0.3333333333333333, 0.9166666666666666, 0.8461538461538461, 1.0, 0.9166666666666666, 1.0, 0.8333333333333334, 0.9230769230769231, 0.75, 1.0]\n",
      "0.1484375\n",
      "0.078125\n",
      "tensor([500.1884], grad_fn=<SubBackward0>)\n",
      "22 [0.8333333333333334, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 0.9166666666666666, 0.6923076923076923, 1.0]\n",
      "0.0703125\n",
      "0.03125\n",
      "tensor([500.0611], grad_fn=<SubBackward0>)\n",
      "23 [1.0, 0.8333333333333334, 1.0, 0.9166666666666666, 1.0, 1.0, 0.9230769230769231, 1.0, 1.0, 1.0]\n",
      "0.0390625\n",
      "0.03125\n",
      "tensor([500.0231], grad_fn=<SubBackward0>)\n",
      "24 [1.0, 0.9230769230769231, 1.0, 0.9166666666666666, 0.9166666666666666, 1.0, 0.9166666666666666, 1.0, 1.0, 1.0]\n",
      "0.015625\n",
      "0.0546875\n",
      "tensor([500.0434], grad_fn=<SubBackward0>)\n",
      "25 [0.9230769230769231, 0.9166666666666666, 1.0, 1.0, 1.0, 0.9230769230769231, 1.0, 0.8333333333333334, 0.8333333333333334, 1.0]\n",
      "0.0546875\n",
      "0.0546875\n",
      "tensor([500.0555], grad_fn=<SubBackward0>)\n",
      "26 [1.0, 0.8333333333333334, 1.0, 1.0, 0.9166666666666666, 0.9166666666666666, 1.0, 0.8333333333333334, 0.9166666666666666, 1.0]\n",
      "0.0234375\n",
      "0.03125\n",
      "tensor([500.0427], grad_fn=<SubBackward0>)\n",
      "27 [1.0, 0.9166666666666666, 0.9230769230769231, 0.9166666666666666, 1.0, 0.9166666666666666, 1.0, 1.0, 1.0, 1.0]\n",
      "0.0390625\n",
      "0.0390625\n",
      "tensor([500.0263], grad_fn=<SubBackward0>)\n",
      "28 [1.0, 1.0, 0.9230769230769231, 0.9166666666666666, 1.0, 0.9166666666666666, 1.0, 1.0, 0.8333333333333334, 1.0]\n",
      "0.0390625\n",
      "0.0390625\n",
      "tensor([500.0337], grad_fn=<SubBackward0>)\n",
      "29 [1.0, 0.8333333333333334, 1.0, 1.0, 1.0, 0.9166666666666666, 1.0, 1.0, 0.8461538461538461, 1.0]\n",
      "0.03125\n",
      "0.0234375\n",
      "tensor([500.0266], grad_fn=<SubBackward0>)\n",
      "30 [1.0, 0.9166666666666666, 1.0, 1.0, 0.9230769230769231, 0.9230769230769231, 1.0, 1.0, 1.0, 1.0]\n",
      "0.0546875\n",
      "0.046875\n",
      "tensor([500.0333], grad_fn=<SubBackward0>)\n",
      "31 [0.9230769230769231, 0.8461538461538461, 0.9166666666666666, 0.9230769230769231, 1.0, 0.9166666666666666, 1.0, 1.0, 1.0, 1.0]\n",
      "0.0\n",
      "0.0078125\n",
      "tensor([500.0273], grad_fn=<SubBackward0>)\n",
      "32 [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9230769230769231, 1.0, 1.0, 1.0]\n",
      "0.03125\n",
      "0.0234375\n",
      "tensor([500.0077], grad_fn=<SubBackward0>)\n",
      "33 [1.0, 0.9166666666666666, 1.0, 1.0, 1.0, 1.0, 0.9166666666666666, 1.0, 0.9230769230769231, 1.0]\n",
      "0.0234375\n",
      "0.0390625\n",
      "tensor([500.0396], grad_fn=<SubBackward0>)\n",
      "34 [0.9230769230769231, 0.9166666666666666, 0.9230769230769231, 1.0, 1.0, 1.0, 1.0, 0.8333333333333334, 1.0, 1.0]\n",
      "0.0546875\n",
      "0.0546875\n",
      "tensor([500.0361], grad_fn=<SubBackward0>)\n",
      "35 [1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 0.9166666666666666, 0.9166666666666666, 0.8333333333333334, 1.0]\n",
      "0.0625\n",
      "0.0625\n",
      "tensor([500.0460], grad_fn=<SubBackward0>)\n",
      "36 [0.8333333333333334, 0.8461538461538461, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 0.9230769230769231, 1.0]\n",
      "0.0390625\n",
      "0.046875\n",
      "tensor([500.0479], grad_fn=<SubBackward0>)\n",
      "37 [0.9230769230769231, 0.7692307692307693, 1.0, 0.9166666666666666, 1.0, 1.0, 1.0, 1.0, 0.9166666666666666, 1.0]\n",
      "0.015625\n",
      "0.0078125\n",
      "tensor([500.0333], grad_fn=<SubBackward0>)\n",
      "38 [1.0, 1.0, 1.0, 0.9166666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "0.0234375\n",
      "0.0234375\n",
      "tensor([500.0193], grad_fn=<SubBackward0>)\n",
      "39 [1.0, 0.8333333333333334, 0.9166666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "0.03125\n",
      "0.0390625\n",
      "tensor([500.0122], grad_fn=<SubBackward0>)\n",
      "40 [0.9166666666666666, 0.7692307692307693, 1.0, 0.9166666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "0.015625\n",
      "0.015625\n",
      "tensor([500.0389], grad_fn=<SubBackward0>)\n",
      "41 [1.0, 1.0, 0.9166666666666666, 0.9230769230769231, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "0.03125\n",
      "0.0234375\n",
      "tensor([500.0099], grad_fn=<SubBackward0>)\n",
      "42 [1.0, 0.8461538461538461, 1.0, 0.9166666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "0.0\n",
      "0.0\n",
      "tensor([500.0089], grad_fn=<SubBackward0>)\n",
      "43 [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "0.0078125\n",
      "0.0\n",
      "tensor([499.9961], grad_fn=<SubBackward0>)\n",
      "44 [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "0.0\n",
      "0.0\n",
      "tensor([499.9961], grad_fn=<SubBackward0>)\n",
      "45 [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "0.0078125\n",
      "0.0078125\n",
      "tensor([500.0001], grad_fn=<SubBackward0>)\n",
      "46 [1.0, 0.9230769230769231, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "0.03125\n",
      "0.0390625\n",
      "tensor([500.0093], grad_fn=<SubBackward0>)\n",
      "47 [0.8571428571428571, 0.9090909090909091, 0.9090909090909091, 1.0, 1.0, 1.0, 0.9333333333333333, 1.0, 1.0, 1.0]\n",
      "0.015625\n",
      "0.015625\n",
      "tensor([500.0464], grad_fn=<SubBackward0>)\n",
      "48 [1.0, 1.0, 0.8461538461538461, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "0.015625\n",
      "0.015625\n",
      "tensor([499.9887], grad_fn=<SubBackward0>)\n",
      "49 [0.9166666666666666, 0.9230769230769231, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "0.015625\n",
      "0.0\n",
      "tensor([500.0147], grad_fn=<SubBackward0>)\n",
      "50 [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "0.0\n",
      "0.0\n",
      "tensor([499.9960], grad_fn=<SubBackward0>)\n",
      "51 [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "0.0\n",
      "0.0\n",
      "tensor([499.9960], grad_fn=<SubBackward0>)\n",
      "52 [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "0.0078125\n",
      "0.0\n",
      "tensor([499.9960], grad_fn=<SubBackward0>)\n",
      "53 [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "0.0\n",
      "0.0\n",
      "tensor([499.9960], grad_fn=<SubBackward0>)\n",
      "54 [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "0.015625\n",
      "0.015625\n",
      "tensor([500.0041], grad_fn=<SubBackward0>)\n",
      "55 [1.0, 1.0, 1.0, 1.0, 1.0, 0.9166666666666666, 0.9166666666666666, 1.0, 1.0, 1.0]\n",
      "0.015625\n",
      "0.0078125\n",
      "tensor([500.0084], grad_fn=<SubBackward0>)\n",
      "56 [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9230769230769231, 1.0, 1.0, 1.0]\n",
      "0.0078125\n",
      "0.0078125\n",
      "tensor([499.9983], grad_fn=<SubBackward0>)\n",
      "57 [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9166666666666666, 1.0]\n",
      "0.0\n",
      "0.0\n",
      "tensor([500.0046], grad_fn=<SubBackward0>)\n",
      "58 [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "0.0234375\n",
      "0.0234375\n",
      "tensor([500.0137], grad_fn=<SubBackward0>)\n",
      "59 [1.0, 0.8333333333333334, 0.9166666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "0.0\n",
      "0.0\n",
      "tensor([500.0018], grad_fn=<SubBackward0>)\n",
      "60 [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "0.0\n",
      "0.0\n",
      "tensor([499.9959], grad_fn=<SubBackward0>)\n",
      "61 [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "0.015625\n",
      "0.015625\n",
      "tensor([500.0171], grad_fn=<SubBackward0>)\n",
      "62 [1.0, 1.0, 0.8333333333333334, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "0.0\n",
      "0.0078125\n",
      "tensor([499.9834], grad_fn=<SubBackward0>)\n",
      "63 [0.9166666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "0.0\n",
      "0.0\n",
      "tensor([500.0107], grad_fn=<SubBackward0>)\n",
      "64 [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "0.015625\n",
      "0.015625\n",
      "tensor([499.9943], grad_fn=<SubBackward0>)\n",
      "65 [0.9230769230769231, 0.9230769230769231, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "0.0078125\n",
      "0.0078125\n",
      "tensor([500.0154], grad_fn=<SubBackward0>)\n",
      "66 [1.0, 1.0, 1.0, 0.9230769230769231, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "0.0\n",
      "0.0\n",
      "tensor([500.0016], grad_fn=<SubBackward0>)\n",
      "67 [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "0.0\n",
      "0.0\n",
      "tensor([499.9959], grad_fn=<SubBackward0>)\n",
      "68 [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "0.0\n",
      "0.0\n",
      "tensor([499.9959], grad_fn=<SubBackward0>)\n",
      "69 [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "0.0\n",
      "0.0\n",
      "tensor([499.9959], grad_fn=<SubBackward0>)\n",
      "70 [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "0.0\n",
      "0.0\n",
      "tensor([499.9958], grad_fn=<SubBackward0>)\n",
      "71 [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "0.0\n",
      "0.0\n",
      "tensor([499.9958], grad_fn=<SubBackward0>)\n",
      "72 [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1369]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m actor = Actor(\u001b[32m10\u001b[39m,\u001b[32m10\u001b[39m)\n\u001b[32m      4\u001b[39m critic = Critic(\u001b[32m10\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mactor_critic\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43mactor\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcritic\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1368]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mactor_critic\u001b[39m\u001b[34m(env, actor, critic, episodes, max_steps, gamma, lr_actor, lr_critic)\u001b[39m\n\u001b[32m     21\u001b[39m old_log_prob = torch.log(action_probabilities)\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m#print(action)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m reward, next_state = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#рассчитываем награду и next_state, где next_state - ndarray процента ошибок по классам на тестовой выборке из batch_size элементов\u001b[39;00m\n\u001b[32m     24\u001b[39m next_state_tensor = torch.FloatTensor(next_state)\u001b[38;5;66;03m#создаем тензор от next_state\u001b[39;00m\n\u001b[32m     25\u001b[39m new_action_probabilities = actor(next_state_tensor)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1365]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mDataSelectionEnv.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m     41\u001b[39m test_dataloader = get_dataloader(test_subset) \u001b[38;5;66;03m#создаем dataloader для тестирования\u001b[39;00m\n\u001b[32m     43\u001b[39m prev_acc, _ = \u001b[38;5;28mself\u001b[39m.evaluate(test_dataloader) \u001b[38;5;66;03m#вычиляем текущую точность модели на тестовой выборке\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#тренируем модель на тренировчной выборке\u001b[39;00m\n\u001b[32m     45\u001b[39m new_acc, err_per_cl = \u001b[38;5;28mself\u001b[39m.evaluate(test_dataloader) \u001b[38;5;66;03m#проверяем модель после тренировки на тестовой выборке\u001b[39;00m\n\u001b[32m     47\u001b[39m reward = new_acc \u001b[38;5;66;03m#вычисляем награду\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1365]\u001b[39m\u001b[32m, line 74\u001b[39m, in \u001b[36mDataSelectionEnv.train_model\u001b[39m\u001b[34m(self, dataloader)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;66;03m#print(output)\u001b[39;00m\n\u001b[32m     73\u001b[39m loss = \u001b[38;5;28mself\u001b[39m.criterion(output, labels-\u001b[32m1\u001b[39m) \u001b[38;5;66;03m#вычислили ошибку с помощью CrossEntropyLoss\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[38;5;28mself\u001b[39m.optim.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\VSCode\\project\\CV_project\\a2c-ppo\\my_venv\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\VSCode\\project\\CV_project\\a2c-ppo\\my_venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\VSCode\\project\\CV_project\\a2c-ppo\\my_venv\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "env = DataSelectionEnv()\n",
    "#print(env.)\n",
    "actor = Actor(10,10)\n",
    "critic = Critic(10)\n",
    "actor_critic(env,actor,critic,episodes=1,max_steps=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
